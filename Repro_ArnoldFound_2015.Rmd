---
title: "**Assessing the reproducibility of microbiome data analysis**"
#author: Patrick D. Schloss, PhD
#date: '`r format(Sys.Date(), format="%B %d, %Y")`'
bibliography: references.bib
output:
  pdf_document:
    includes:
      in_header: header.tex
csl: american-society-for-microbiology.csl
fontsize: 11pt
geometry: margin=1.0in
---


```{r citation_setup, echo=FALSE}
#install.packages("knitcitations")
library(knitcitations)
cleanbib()
options("citation_format" = "pandoc")
```

Today's most significant health care concerns include the treatment and prevention of obesity, diabetes, autism, antibacterial resistance, cancer, eczema, periodontitis, bacterial vaginisis, and a growing list of other "Diseases of affluence". There is growing sentiment that the bacteria that live in and on the human body (i.e. the human *microbiota*) and the environment they inhabit (i.e. the human *microbiome*) are at least partially involved in these diseases `r citep(c("10.1053/j.gastro.2014.02.009", "10.1016/j.cgh.2013.03.038", "10.1111/j.1753-4887.2012.00505.x"))`. One hypothesis is that changes in diet and hygiene have altered our microbiota leading to the increased prevalence of these disorders `r citep(c("10.1136/bmj.299.6710.1259", "10.1038/nrmicro2245"))`. Clearly, the human microbiome and the conditions and diseases it affects have profound implications for public policy and health care. Fueled by the decreasing costs of DNA sequencing and significant investments by the US National Institutes of Health and private foundations (e.g. `r citet("10.1038/nature11234")`), the study of the human microbiome has exploded over the past 15 years. In 2010, 78 papers were published that included the keywords "microbiota" or "microbiome". In 2014, there were over 4,000. Over the past 10 years the median annual increase in number of publications in the microbiome research space has been 36% while has only been 5% for cancer. Although there is considerable excitement about this new area of research, there is also healthy skepticism that many of the claims made by its proponents are exaggerated `r citep(c("10.1038/512247a", "10.1186/s13073-015-0143-5"))`.

This explosion has created a unique scientific environment. Microbiologists who have been classically trained as reductionist molecular biologists are now expected to be molecular ecologists integrating the tools of statistics, bioinformatics, ecology, and clinical science. For the past 13 years we have developed software that is now the most widely used software package in the field and worked with numerous researchers. Our experience suggests that many new researchers face considerable difficulties implementing analysis plans in a manner that is reproducible. Furthermore, because novices do not have the ability to discriminate between competing methods, they may not perform the ideal methods. Even when analyses are done well, it is common for a series of complex data manipulation steps distilled to a single sentence in the methods section of a paper. Obviously, the field needs to assess the overall reproducibility and robustness of published data analyses and it needs to create a mechanism to better train novice researchers.

The Arnold Foundation has a special interest in "Research Integrity" and understanding how problems with research integrity affect public health policy. We are confident that this project fits within the mission of the foundation. As outlined above, the human microbiome is central to many issues in current public health discussions. The mission of the proposed project would be to assess the reproducibility of microbiome research by training researchers in the best current practices that improve reproducibility. To achieve our goals, we propose the following objectives:

* **Objective 1. Assess the level of methodological transparency of microbiome studies**
* **Objective 2. Quantify the ability of researchers to reproduce reported analyses**
* **Objective 3. Determine whether similar microbiome studies validate each other**

Together, these objectives will allow us to assess whether microbiome research has a reproducibility problem. As outlined below, we will achieve these objectives by engaging the microbiome research community. Given the significant role of the microbiome in human health improving the reliability of the results form the proposed research will have a meaningful positive impact.

\newpage


## Problem definition
Our research group has been at the forefront of microbiome research through our development of the popular software package, mothur, which has now been cited more than 2,600 times, making it the most widely cited software package for analyzing microbiome data `r citep("10.1128/AEM.01541-09")`. This has allowed us to have a significant role in helping to train literally thousands of microbiome researchers. Overall, these experiences have allowed us to identify three critical problems in the field that relate to the problem of reproducibility.

***Lack of training in data analysis.*** We recently conducted a survey of mothur users to better understand their background. Of respondents, 41% were graduate students, 41% were PhD-level scientists, and 12% were faculty. Microbiome research is clearly being performed across training levels. Perhaps most surprising, 41% of the respondents had no programming experience. These results and our experiences emphasize that most individuals carrying out microbiome research have limited experience and are self-taught. There is a growing set of resources to overcome some of these problems including workshops (e.g. Software and Data Carpentry), literate programming tools (e.g. knitr and IPython), and online instruction (e.g. Codecademy). However, these efforts do not include capstone projects where learners can apply their training to a large, domain-specfic project. Our group is beginning to develop a microbiome-specific curriculum to improve research reproducibility and it is clear that there is considerable enthusiasm for this type of training.

***Lack of access to original data and code from published studies.*** Even if a researcher was trained in the best practices of microbiome research, the fact that many investigators do not make their raw data publicly accessible, even though it is required by the journals and funding agencies, is a significant problem. As a small example, we are interested in understanding the differences between the microbiota found in people living in a Western culture and that of people in these different indigenous communities. Three of the four recent papers describing the microbiota of indigenous peoples have yet to make their raw data publicly accessible `r citep(c("10.1038/ncomms4654", "10.1038/ncomms7505", "10.1016/j.celrep.2015.03.049", "10.1126/sciadv.1500183"))`. We have only been able to acquire one of the three non-public datasets from the authors `r citep("10.1038/ncomms4654")`. Clearly, our ability to address this question has been stalled. The lack of transparency extends to the publishing of incomplete methods descriptions that are frequently at odds with the papers being referenced and the use of "in house" scripts and pipelines that are not disseminated with the final manuscript. The field would be well served by an effort to quantify the accessibility of raw data and the code and methods used to analyze those data.

***Inter-study reproducibility.*** A larger and perhaps more important issue is the lack of explicit validation of observations using different populations of people. For example, one of the keystone results of microbiome research has been the association between the composition of the gut microbiota with obesity. It was initially observed that obesity was negatively with microbiota diversity and the ratio of the bacterial phyla Bacteroidtes and Firmicutes in the gut `r citep("10.1038/nature05414")`. Two independent groups have since used additional published data to indicate that these observations could not be validated `r citep(c("10.1371/journal.pone.0084689", "10.1016/j.febslet.2014.09.039"))`. Given the great fanfare that much microbiome research has been greeted with, we need to expand the scope of this effort to validate studies related to the definition of health, colorectal cancer, inflammatory bowel diseases, cystic fibrosis, and others.

Considering the importance of microbiome research in understanding many diseases with major public policy implications, there is a great need to directly address the problems related to inadequate training, limited access to data and methods, and an absence of a culture that attempts to validate results in different populations of subjects. The three innovative objectives outlined below will address these issues.


\newpage

## Objective 1. Assess the level of methodological transparency of microbiome studies

Two challenges affect one's ability to reproduce the analysis of others. First, it is well-established that restrictions on the length of papers and the overall format of research papers results in abbreviated descriptions of analytical methods `r citep(c("10.1038/505612a", "10.1186/2049-2618-2-8"))`. Frequently, the methods describing the data curation steps in microbiome analyses are distilled to at most two or three sentences that cite other studies. In reality, the process of manipulating raw sequence data into a format that can be used to address specific biological questions are complex and depend on myriad options that frequently go undocumented. In some cases papers are cited that actually describe multiple methods and so it becomes unclear what was actually done.  Second, journals and funding agencies have made great strides to require that researchers make their raw data publicly accessible `r citep(c("10.1038/505612a", "10.1126/science.aaa1724"))`. Unfortunately, these requirements involve researchers self-reporting and rarely involve independent confirmation that database accession numbers are provided or that the links are live. Clearly, if data are not publicly available, it will be impossible for other researchers to reproduce and build upon the initial work. This limits the checks and balances in science and limits the progress of the field. Together, these two problems limit the transparency of the scientific process. To solve these problems it is important that we first quantify the actual practices among microbiome researchers.

To quantify these practices, we will perform an audit of microbiome papers. Although it is likely that additional questions will evolve as we begin to screen papers, our preliminary list of questions will include:

* Is there a description of where the raw data are deposited?
* Are the data actually available at the stated location?
* How many sentences are devoted to describing data processing?
* Are the actual commands available as supplementary material or in a repository?

This audit will be performed by creating a paper crawler written in the R programming language. It will query PubMed for microbiome papers that generate sequence data, retrieve html versions of those papers, and then parse the html to answer these questions. Human oversight will confirm that the results retrieved by the paper crawler are correct. With the results of this screen, we will assess whether the results vary by the scope of the journal, the journal's impact factor, the number of microbiome papers previously published by the authors, and the scope of the research question. Our *a priori* hypothesis is that there is a general lack of transparency in describing methods and making data available. Disseminating these results will provide a benchmark for where the microbiome literature is currently that can be used to quantify the evolution of practices over time. Furthermore, by developing a rubric to assess reproducibility we will create a badging system that we can assign to microbiome studies that describe their reproducibility practices.


## Objective 2. Quantify the ability of researchers to reproduce reported analyses

The limited space allotted to authors for describing their methods is portrayed as one of the factors that limits the ability of others to implement new methods and reproduce the work of others. Yet, the foundation of science is the ability to reproduce the work of others and then take the next step. To circumvent these limitations, a small, but growing number of researchers, including ourselves, are releasing the code that was used to convert raw sequencing data to the final results. Using tools such as R-based knitr documents and IPython/Jupyter notebooks it is possible to deposit these materials on public repositories such as GitHub `r citep("10.1038/515151a")`. Because GitHub is based on the version control software git, it is even possible to see the complete history of the data analysis process. Although anecdotal, we have received several emails from individuals that have explored our repositories (https://github.com/SchlossLab) to learn how to adapt our analysis to their research questions. Others have indicated that this transparency has led them to use our data to address their specific questions. By presenting a fully reproducible workflow alongside our papers we have made it easier for other researchers to reproduce our work. This approach presents a positive solution to the problem of reproducibility. Currently, however, we do not know how reproducible the analyses reported in the microbiome literature are. Although the lack of reproducibility is clearly a negative, it is important to quantify the level of reproducibility in the field and the covariates that affect reproducibility so that it will be possible to improve the field.

The goal of this objective is to host "reproducibility parties" where participants will reimplement the methods described in published papers to determine whether they can reproduce the stated results. The results of this objective will further the training goals of improving the reporting of methods used in the microbiome literature and quantify the level of a perceived reproducibility problem within the microbiome literature. Through these reproducibility parties, we seek to address three specific questions:

* What percentage of papers can be reproduced by other researchers?
* What factors covary with reproducibility (e.g. journal, bioinformatics training of authors, previous microbiome publications, etc.)?
* Does the ability of an individual to reproduce the work depend on characteristics of that individual (e.g. training history, previous microbiome publications, etc.)?

We will broadly recruit an international and diverse cohort of microbiome researchers to participate in several in-person and virtual reproducibility parties. All cohort members will be asked to complete autotutorial materials that are being developed with NIH funding to train researchers in the best practices of performing reproducible microbiome data analysis. We will identify a collection of 25-50 microbiome papers that provided their raw data. Then members of the cohort will be assigned to one of the datasets and asked to address one specific claim from the paper while doing their best to follow the methods described in the paper. We will randomly assign three individuals to each paper so we can assess variation in the ability of cohort members to implement the methods and individuals will be able to work on multiple datasets. Cohort members will be blinded to the identity of the authors of each paper and to the identity of the other individuals participating analyzing the specific claim. Finally, all participants will be asked to use reproducible methods to document their data analysis steps and to submit that documentation to the project's GitHub account. Four reproducibility parties will be hosted at the University of Michigan. Those unable to travel to Ann Arbor, MI will be able to participate at their home institution and interact with the project using google Hangouts and other networking tools. At the end of the study, all of the repositories within the project's GitHub account will be made public and the results published with all cohort members as co-authors. Based on our experience with a large number of microbiome investigators, our *a priori* hypothesis is that at least 25% of published research cannot be reproduced independently from the original researchers. Beyond assessing the reproducibility of the microbiome literature we anticipate that this objective will improve the training and practices of the cohort members.



## Objective 3. Determine whether similar microbiome studies validate each other

Problems related to reproducibility in science have come to the forefront because of studies such as those carried out by Bayer, Amgen, and others who attempted to repeat the entire experimental design of previous biomedical research studies `r citep(c("10.1038/483531a", "10.1038/nrd3439-c1"))`. In general, these efforts suggest a reproducibility crisis in science. Although the issue of reproducibility has been broached within the field of microbiome research `r citep("10.1186/2049-2618-2-8")`, a large scale replication effort has yet to be proposed. Due to the explosive growth of the microbiome field, it appears that replication studies are already being performed informally. In general, authors will acknowledge the previous results of a study, but will not dig into the actual data to see whether the actual observations of their study are confirmed by previous studies. Unfortunately, there are only isolated cases where the results of prior studies are synthesized with the new data to determine whether the claims of each study are supported by the others. Two independent meta-analyses have investigated the role of the microbiota in obesity. Both rejected the initial reports out of the Gordon lab, which implicated differences in diversity and the ratio of Bacteroidetes to Firmicutes were not been replicated in other cohorts `r citep(c("10.1371/journal.pone.0084689", "10.1016/j.febslet.2014.09.039"))`. As critics of replication studies correctly point out there are many reasons for why a study might fail to replicate. These include use of different protocols across studies, non-standard definitions, and differences in the underlying populations being studied. Regardless, if a signal is biologically relevant, we would expect it to transcend these issues.

The goal of this objective is to apply a common set of methods to determine whether overlapping studies replicate each other. Examples include validation of microbiome-derived biomarkers associated with:

* Colorectal cancer `r citep(c("10.1158/1940-6207.CAPR-14-0129","10.3945/ajcn.112.046607","10.1371/journal.pone.0039743","10.1038/ismej.2011.109"))`
* Inflammatory bowel diseases including Crohn's disease and colitis
`r citep(c("10.1186/s40168-015-0084-7", "10.1038/ctg.2014.21", "10.1136/gutjnl-2014-308341"))`
* *Clostridium difficile* infection `r citep(c("10.1128/mBio.01021-14", "10.1186/s40168-015-0070-0", "10.1016/j.anaerobe.2015.03.008"))`
* Differences between Western and "primitive" or indigenous populations `r citep(c("10.1038/ncomms4654", "10.1038/ncomms7505", "10.1016/j.celrep.2015.03.049", "10.1126/sciadv.1500183"))`

For example, our research group did not observe a difference in the diversity of the gut microbiome of healthy individuals and those with colorectal cancer, although, others have. By analyzing multiple datasets that have characterized similar cohorts we will be able to determine whether the difference in diversity is real. This objective will also give us an opportunity to analyze a large number of datasets using a common data analysis approach. This will allow us to ascertain how sensitive observations are to variation in approach. While the goal of Objective 2 is to determine whether the stated methods are reproducible, the goal of this objective will be to determine whether results are reproducible across studies when a common, well-justified approach is applied to the raw data.



## Summary

The development of new sequencing technologies has fueled interest in the human microbiome as a key component in transitions between health and disease. The explosion of this research area has resulted in the generation of many large datasets, provocative results, and an unease that the role of the microbiome may be overstated. By completing the proposed objectives, we will be able to quantify the extent of a reproducibility problem in the field. Perhaps most promising is the prospect of using these objectives to educate novices and lead the field in the best practices for insuring that future analyses are more reproducible.















\newpage

## References

```{r references, echo=FALSE, message=FALSE}
write.bibtex(file="references.bib")
```
